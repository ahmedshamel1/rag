# ğŸ§  Multi-Agent RAG Chatbot System

A college project exploring the power of **multi-agent systems** and **Retrieval-Augmented Generation (RAG)** using cutting-edge tools like **LangChain**, **Ollama**, and **ChromaDB**. This chatbot leverages multiple specialized agents to collaboratively process user queries, fetch context from PDFs/web sources, and generate accurate, context-aware responses.

---

## ğŸ§‘â€ğŸ’» Meet the Agents

This system uses a **multi-agent design** with three specialized agents, each focused on a unique domain of knowledge:

### 1. ğŸ§  General Agent
Handles open-domain, casual, or general-purpose questions. This is the default fallback agent if no specific domain is detected in the query.

### 2. ğŸ¤– AI Specialist Agent
Focused on answering questions related to Artificial Intelligenceâ€”LLMs, machine learning concepts, tools like LangChain, model training, etc.

### 3. ğŸ“ Concordia Helpdesk
Provides information about the **Computer Science Department at Concordia University**, such as:
- Course offerings
- Faculty details
- Admission requirements
- Labs and facilities

Each query is **routed dynamically** based on its content. The **controller agent** analyzes the question and delegates it to the most relevant specialist, ensuring high-quality, domain-specific answers.


## ğŸš€ Features

- ğŸ¤– **Multi-Agent Architecture**: Agents with specialized roles (Retriever, Summarizer, Answerer, etc.)
- ğŸ“š **RAG Pipeline**: Combines real-time document/web data with LLM responses
- ğŸ“„ **PDF + Web Scraping Support**: Extracts context from uploaded files or URLs
- ğŸ” **Vector Search**: Uses ChromaDB for semantic document retrieval
- ğŸ§  **Local LLMs**: Runs large language models locally using Ollama
- ğŸŒ **FastAPI Backend**: Lightweight and scalable RESTful API
- ğŸ¨ **Streamlit Frontend**: Simple, interactive UI for users to chat with the system

---

## ğŸ› ï¸ Tech Stack

| Layer         | Tool/Framework       | Description                                      |
|--------------|----------------------|--------------------------------------------------|
| LLM Orchestration | LangChain             | Multi-agent coordination + RAG pipeline         |
| LLM Runtime   | Ollama                | Run open-source LLMs like LLaMA locally         |
| Memory Store  | ChromaDB              | Vector database for storing and retrieving docs |
| Backend       | FastAPI               | Serves the API endpoints                        |
| Frontend      | Streamlit             | Provides a user-friendly chat interface         |
| Data Sources  | Web Scraping + PDF    | Collects external data for grounding responses  |

---

## ğŸ“‚ Project Structure
    
    multi-agent-chatbot/
    â”œâ”€â”€ backend/
    â”‚   â”œâ”€â”€ chat-agents/            # LangChain agent logic
    |       â”œâ”€â”€ ai_llmservice.py  
    |       â””â”€â”€ concordia_llm_service.py 
    |       â””â”€â”€ general_assistant.py
    |   â”œâ”€â”€ utils/
    â”‚   â””â”€â”€ GeneralAgentSARSA.py  # Reinforcement Learning Model
    â”œâ”€â”€ database/                # Chroma db which contains embeddings
    |   â””â”€â”€ chroma_general/
    |   â””â”€â”€ chroma_ai/
    |   â””â”€â”€ chroma_concordia/
    â”œâ”€â”€ frontend/
    â”‚   â””â”€â”€ chat_app.py           # Streamlit UI
    â”œâ”€â”€ assests/
    â”‚   â””â”€â”€ ai_book_1.pdf         # PDFs or scraped data
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ app_server.py             # FastAPI Server
    â””â”€â”€ README.md


---

## ğŸ§ª Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/multi-agent-chatbot.git
cd multi-agent-rag-chatbot-system
```

### 2. Setting up Virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
```
### 3. Install Requirements
```bash
pip install requirement.txt
```
### 4. Start Ollama (LLM Server) and Run the backend

```bash
ollama run llama3
cd backend
uvicorn main:app --reload
```

## Architecure Diagram

![LANGCHAIN](https://github.com/user-attachments/assets/3de77a06-53fc-453b-8071-b4c00f41a3db)

## ğŸ§  How It Works

1. The user enters a query in the **Streamlit** frontend.
2. The query is sent to the **FastAPI** backend.
3. A **controller agent** built using **LangChain** orchestrates the process:
    - ğŸ§¾ **Retriever Agent**: Queries **ChromaDB** for relevant information from preprocessed PDFs and web pages.
    - ğŸŒ **Scraper/Loader Agent**: If needed, scrapes content from provided URLs or loads and parses uploaded PDFs.
    - ğŸ§  **Answer Generator Agent**: Uses an **LLM via Ollama** to craft a final, context-aware response based on retrieved data.
4. The response is returned via the API and displayed in the Streamlit chat interface.

## ğŸ“¸ Screenshots

### Home Page with access to multiple agents:
<img width="1335" alt="image" src="https://github.com/user-attachments/assets/e53c95da-3768-4859-9ac9-6a30448b6b8a" />

The chatbot uses **ConversationalMemoryBuffer** to understand the context from the previous chat history.
<img width="1335" alt="image" src="https://github.com/user-attachments/assets/cc3f9fa5-c755-4d7b-8205-4084b0f3f367" />











